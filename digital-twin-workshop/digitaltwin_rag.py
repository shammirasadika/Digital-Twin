"""
Digital Twin RAG Application
Based on Binal's production implementation
- Upstash Vector: Built-in embeddings and vector storage
- Groq: Ultra-fast LLM inference
"""

import os
import json
from dotenv import load_dotenv
from upstash_vector import Index
from groq import Groq


# Load environment variables
load_dotenv()
print("[DEBUG] UPSTASH_VECTOR_REST_TOKEN:", os.getenv("UPSTASH_VECTOR_REST_TOKEN"))

# Constants
JSON_FILE = "digitaltwin.json"
GROQ_API_KEY = os.getenv('GROQ_API_KEY')
DEFAULT_MODEL = "llama-3.1-8b-instant"

def setup_groq_client():
    """Setup Groq client"""
    if not GROQ_API_KEY:
        print("‚ùå GROQ_API_KEY not found in .env file")
        return None
    try:
        client = Groq(api_key=GROQ_API_KEY)
        print("‚úÖ Groq client initialized successfully!")
        return client
    except Exception as e:
        print(f"‚ùå Error initializing Groq client: {str(e)}")
        return None

def setup_vector_database():
    """Setup Upstash Vector database with built-in embeddings"""
    print("üîÑ Setting up Upstash Vector database...")
    try:
        index = Index.from_env()
        print("‚úÖ Connected to Upstash Vector successfully!")
        # Check current vector count
        try:
            info = index.info()
            current_count = getattr(info, 'vector_count', 0)
            print(f"üìä Current vectors in database: {current_count}")
        except:
            current_count = 0
        # Load data if database is empty
        if current_count == 0:
            print("üìù Loading your professional profile...")
            try:
                with open(JSON_FILE, "r", encoding="utf-8") as f:
                    profile_data = json.load(f)
            except FileNotFoundError:
                print(f"‚ùå {JSON_FILE} not found!")
                return None
            # Prepare vectors from content chunks
            vectors = []
            content_chunks = profile_data.get('content_chunks', [])
            if not content_chunks:
                print("‚ùå No content chunks found in profile data")
                return None
            for chunk in content_chunks:
                enriched_text = f"{chunk['title']}: {chunk['content']}"
                vectors.append((
                    chunk['id'],
                    enriched_text,
                    {
                        "title": chunk['title'],
                        "type": chunk['type'],
                        "content": chunk['content'],
                        "category": chunk.get('metadata', {}).get('category', ''),
                        "tags": chunk.get('metadata', {}).get('tags', [])
                    }
                ))
            # Upload vectors
            index.upsert(vectors=vectors)
            print(f"‚úÖ Successfully uploaded {len(vectors)} content chunks!")
        return index
    except Exception as e:
        print(f"‚ùå Error setting up database: {str(e)}")
        return None

def query_vectors(index, query_text, top_k=3):
    """Query Upstash Vector for similar vectors"""
    try:
        results = index.query(
            data=query_text,
            top_k=top_k,
            include_metadata=True
        )
        return results
    except Exception as e:
        print(f"‚ùå Error querying vectors: {str(e)}")
        return None

def generate_response_with_groq(client, prompt, model=DEFAULT_MODEL):
    """Generate response using Groq"""
    try:
        completion = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": "You are an AI digital twin. Answer questions as if you are the person, speaking in first person about your background, skills, and experience."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.7,
            max_tokens=500
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        return f"‚ùå Error generating response: {str(e)}"

def rag_query(index, groq_client, question):
    """Perform RAG query using Upstash Vector + Groq"""
    try:
        # Step 1: Query vector database
        results = query_vectors(index, question, top_k=3)
        if not results or len(results) == 0:
            return "I don't have specific information about that topic."
        # Step 2: Extract relevant content
        print("\nüß† Searching your professional profile...\n")
        top_docs = []
        for result in results:
            metadata = result.metadata or {}
            title = metadata.get('title', 'Information')
            content = metadata.get('content', '')
            score = result.score
            print(f"üîπ Found: {title} (Relevance: {score:.3f})")
            if content:
                top_docs.append(f"{title}: {content}")
        if not top_docs:
            return "I found some information but couldn't extract details."
        print(f"‚ö° Generating personalized response...\n")
        # Step 3: Generate response with context
        context = "\n\n".join(top_docs)
        prompt = f"""Based on the following information about yourself, answer the question.
Speak in first person as if you are describing your own background.

Your Information:
{context}

Question: {question}

Provide a helpful, professional response:"""
        response = generate_response_with_groq(groq_client, prompt)
        return response
    except Exception as e:
        return f"‚ùå Error during query: {str(e)}"

def main():
    """Main application loop"""
    print("ü§ñ Your Digital Twin - AI Profile Assistant")
    print("=" * 50)
    print("üîó Vector Storage: Upstash (built-in embeddings)")
    print(f"‚ö° AI Inference: Groq ({DEFAULT_MODEL})")
    print("üìã Data Source: Your Professional Profile\n")
    # Setup clients
    groq_client = setup_groq_client()
    if not groq_client:
        return
    index = setup_vector_database()
    if not index:
        return
    print("‚úÖ Your Digital Twin is ready!\n")
    # Interactive chat loop
    print("ü§ñ Chat with your AI Digital Twin!")
    print("Ask questions about your experience, skills, projects, or career goals.")
    print("Type 'exit' to quit.\n")
    print("üí≠ Try asking:")
    print("  - 'Tell me about your work experience'")
    print("  - 'What are your technical skills?'")
    print("  - 'Describe your career goals'")
    print()
    while True:
        question = input("You: ")
        if question.lower() in ["exit", "quit"]:
            print("üëã Thanks for chatting with your Digital Twin!")
            break
        if question.strip():
            answer = rag_query(index, groq_client, question)
            print(f"ü§ñ Digital Twin: {answer}\n")

if __name__ == "__main__":
    main()
